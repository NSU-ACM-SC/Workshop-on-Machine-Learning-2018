## Workshop-on-Machine-Learning-2018
   This repo contents all the informations regarding workshop on Machine Learning by NSU ACM SC. 
   
   
   
   ## What we are going to learn from this Workshop
   ### Day 1:
   - **Introduction to Machine Learning**
   - **Intuition of the most common ML Algorithms**
   ### Day 2:
   - **Applications of ML**
   - **Pre-processing Data for ML models**
   - **Use of different ML Algorithms**
   - **Implementation of Algorithms**
   ### Day 3:
   - **[Kaggle](https://www.kaggle.com/) Competition**

   

                                           The instructors for this workshop are 

```
                                                     Asif Ahmed Neloy 
```
<p align="center">
  <img width="460" height="450" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/12507265_1651272625133624_6327118509352301225_n.jpg">
</p>

```
                                                     Mostafa Didar
```
<p align="center">
  <img width="460" height="400" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/28234796_10213887325570230_8277411262478695512_o.jpg">
</p>


## Beginner Machine Learning Resources (Forked From - https://github.com/NSU-ACM-SC/Beginner-Machine-Learning-Resources)

### Key things to focus on first

- **Language** : `python3` , you can opt for `python2` and many older tutorials or books are on `python2` but it will soon become obsolete, so its better to focus on `python3`. Yes you can go for `Java` or `C++`, but not recommended for beginners. Python just has better tools these days.

- **Linear Algebra** : You can follow resources given, also NSU offers `MAT125` course on Linear Algebra.

- **Statistics** : Again NSU offers `MAT361` for this purpose, if you're yet to do the course or if you think that you need easier or more elaborate resources you can follow the resource links.

- **Reading a lot of books and the willingness to try them out, implement them writing code** : This is important. You have to keep in mind that Machine Learning as a field is vast and you'll need to study a lot. So the key is to read, write code, discuss among your friends. Discussing is the best way to get things done because you will always meet someone who has some better idea or has a better concept or knows the topic better.

- **Follow people on twitter**: Follow the experts of the field online. Ironically hardly any of them are on facebook so your best bet to find them is twitter. A list of people to follow is given at the end. Also read medium publication **Towards Data Science**. I haven't found any other publication that much organized on this topic.

- **Where to get ideas about projects?**: Github is a good place to start with.

- **Don't waste time running after everything**: Do one thing at a time, it's very tempting to run after cool things people are doing. Fine, you're a beginner, hold your horses, learn things better, then you'll see that you can make things better and  cooler than those guys. ü§ì

- **If things get difficult?**: Things will get difficult at times. So, don't lose hope. Be on your path and things will get just fine üòÑ

- **Practice on Kaggle**: You should practice there. The best place to practice actually but first learn some basics. Kaggle is to machine learning what UVA is to Competitive Programming.

- **If you're Windows**: Some python packages are difficult to install on Windows. No that doesn't mean you have to get Linux or macOS to do things (well things are easier there!). Instead of using Python from their official site, you can download and install [Anaconda](https://www.anaconda.com/download/), a python distribution that comes with all the packages you need to get started with machine learning/ data science/ scientific computing.

### Python Programming Resources
These are probably the best resources to begin with, follow whichever you like, all do the same job though. (Don't go to follow everything at once!)
- [Python3 beginner tutorials by Sentdex](https://www.youtube.com/playlist?list=PLQVvvaa0QuDe8XSftW-RAxdo6OmaeL85M)
- [Python Programming Tutorials by Socratica](https://www.youtube.com/playlist?list=PLi01XoE8jYohWFPpC17Z-wWhPOSuh8Er-)
- [MIT Open Courseware Python Course](https://www.youtube.com/playlist?list=PL57FCE46F714A03BC)
- [edx Python Course](https://www.edx.org/course/introduction-python-absolute-beginner-microsoft-dev236x-1)

- [Automate the boring stuff with python (for people who have never touched python)](https://automatetheboringstuff.com)

- [‡¶∏‡¶π‡¶ú ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶™‡¶æ‡¶á‡¶•‡¶® - ‡ß©](https://legacy.gitbook.com/book/maateen/python/details)

#### Python tools related courses
- [edx Python for Data Science](https://www.edx.org/course/python-for-data-science)
- [Using Python for Research by Harvard edx](https://www.edx.org/course/using-python-research-harvardx-ph526x-0)
- [Sentdex Introduction to Matplotlib for data Visualization](https://www.youtube.com/playlist?list=PLQVvvaa0QuDfefDfXb9Yf0la1fPDKluPF)

##### If you want to use an IDE
Only IDE I can suggest in a heart beat is [Pycharm](https://www.jetbrains.com/pycharm/). The community edition is free and you get the Professional Edition for free if you register with your NSU Email in the [Jetbrains Student Program](https://www.jetbrains.com/student/).  

#### Not an IDE? Text Editor Suggestions?
- [Atom](https://atom.io)
- [Visual Studio Code](https://code.visualstudio.com/)

### Frameworks?
- One rule of thumb is, when you're beginning to learn theoretical topics, it's best not to touch frameworks, use only what is necessary and try writing things from scratch, this way you get to know more and also the problems you face and solve will deepen your understanding on the topic.

- Still there are some python modules you need to use to make life easier. Just use `numpy` for mathematical stuff and `matplotlib` for visualizing graphs.

- When you've a good grasp of things: use `scikit-learn`
- You'll get more information on this in the online courses! üòÖ

### Now for real stuff : Machine Learning resources
#### Online Courses:
- [Introduction to Machine Learning by Andrew NG](https://www.coursera.org/learn/machine-learning)
- [Machine Learning Foundations: A Case Study Approach](https://www.coursera.org/learn/ml-foundations)
- [Applied Machine Learning in Python](https://www.coursera.org/learn/python-machine-learning)
- [Practical Machine Learning](https://www.coursera.org/learn/practical-machine-learning)
- [Machine Learning Fundamentals by UCSanDiego on Edx](https://www.edx.org/course/machine-learning-fundamentals-uc-san-diegox-dse220x)
- [Sentdex - Machine Learning with Python](https://www.youtube.com/playlist?list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v)
- [Udacity Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120)

**So many resources, what to follow eh?** Good question. One thing about learning is that there's nothing called a universal good resource. It depends on the learners what they find best. Start exploring and find out the best one for you. üßê

And this list is not the end of the world, you may find sources that are better!


#### Linear Algebra and Stat
Best and the easiest courses? Follow Khan Academy -
- [Linear Algebra](https://www.khanacademy.org/math/linear-algebra)
- [Stat](https://www.khanacademy.org/math/statistics-probability)

#### Books, Lierature(Journals, Papers)
Some prefer books for then online courses and the problem with books is that different books take different approaches. Thanks to our friends at **NSU ACM SC R&D**, they've created a nice repository of books in digital format - [link here](https://goo.gl/4sBveV). Books ain't cheap, and you won't find every book in the local market as well.

#### Where to find digital copies of other books?
Well distributing digitals copies is kind of illegal but we live in a country where buying a book with 4~5K BDT isn't feasible.

You can use [libgen.io](http://libgen.io) to get books in PDF or ePub format.

If you want to get literature, visit [scihub](https://sci-hub.tw). You can also get IEEE or ACM Library membership (you need to pay yearly for that) to read papers and books (unless you want to use scihub or libgen)

#### Honorary mention
Very few people have the guts to go in depth and discuss things in Bangla, and [Manash vai](https://github.com/manashmndl) is one of them. His online book on machine learning is one of the best and it's in Bangla! And you can read it for free. Also you can follow his projects on his github repo.

- Link : [https://ml.manash.me](https://ml.manash.me)

### A general guideline to begin with
- Start with any of the online courses (other than the case study approach, do that after you have done at least one course).
- Some people understand theories better, for them the Andrew NG course is the best place to start.
- Some people understand theories better when it's discussed in code. So for coder people (like me ü§ì) start with any other course in the list.
- Things may seem overwhelming at times, so do this, drink more water, because human brain works better when it's swimming in a bath of fluid, take breaks, stop punishing yourself, discuss things with people, use groups, search in Google, annoy people who can help you and etc.
- Avoid wannabe people. Machine Learning has gained greater hype than the [Justice League Trailer](https://youtu.be/r9-DM9uBtVI). So obvious there will be no shortage of people online who know very little and will pretend they know a lot. So, avoid those people.  

#### Necessary Tools and keys

- Anaconda installation guide : https://github.com/NeloyNSU/Install-Anaconda-on-Windows-10/blob/master/README.md
- jupyter notebook guide : https://github.com/NeloyNSU/Jupyter-Notebook-Guide-for-Windows-10/blob/master/README.md


#### People and publications to follow on twitter or github
- [Ian Goodfellow](https://twitter.com/goodfellow_ian)
- [Josh Gordon](https://twitter.com/random_forests)
- [Fran√ßois Chollet](https://twitter.com/fchollet)
- [Manash Kumar Mandal](https://github.com/manashmndl)
- [Asif Ahmed Neloy](https://github.com/NeloyNSU)

#### Other Things to follow
- Get active on Quora. Ask questions, search and try to answer.
- [Medium Towards Data Science](https://towardsdatascience.com)


# So lets start Machine Learning


## prerequisite
- **[Install python 3.7](https://github.com/NeloyNSU/Python-pip-Windows-installation)**
- **[Install Anaconda](https://github.com/NeloyNSU/Jupyter-Notebook-Guide-for-Windows-10)**
- **[Update Packages](https://github.com/NeloyNSU/Jupyter-Notebook-Guide-for-Windows-10)**

## Installation Issues
- **[Anaconda install issues](https://github.com/ContinuumIO/anaconda-issues/issues/1581)**
- **[Verify Anaconda Installation](https://stackoverflow.com/questions/48342098/how-to-check-python-anaconda-version-installed-on-windows-10-pc?rq=1)**

## What is Machine Learning?
‚ÄúMachine Learning is the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.‚Äù.

<p align="center">
  <img width="360" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/1_erhS3Y1ZtN3bcJAYgaLC_g.gif">
</p>




## Why Machine Learning?
Machine Learning can be used in detecting anomalies, recommending new products, enhancing customer services and a lot of things. In fact, with the amount of data that is available along with the exceptional computing power machine learning can prove to be extremely useful for future.

More than 80% of companies, around the world, are already planning to use machine learning & AI to enhance their customer experience. There is a lot of innovation happening in the field of machine learning. There is no doubt about the importance of machine learning but the biggest challenge is the lack of expertise. There is a huge potential but the lack of resources is slowing things down a bit.
E.g `Youtube`,`Netflix`, `Spotify recommendations`, `FB instagram ads`, `gmail`, `Disease Identification/Diagnosis`, `Weather Forecasting`, `Video Surveillance`, `Online Customer Support(chatbots)`, `Online fraud detection`, `Autonomous driving`, `Image Recognition`, `Object detection`, `Speech recognition in Alexa`, `Siri` etc.

## How does Machine Learning work?

Machine learning algorithms find natural patterns in data that generate insight and help you make better decisions and predictions. We feed in a set of data and the Machine learns from that data and predicts an output and it can improve that output overtime.

## Tell about our experiences: What are we currently working on..


## Types of Learning:
<p align="center">
  <img width="460" height="350" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/1_9Eu_-DDMZ_bP_t94_MMEYA.png">
</p>


## Supervised Learning: 
Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output.
                          ``` Y = f(X)
                          ```
<p align="center">
  <img width="560" height="350" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/2.png">
</p>

The goal is to approximate the mapping function so well that when you have new input data (x) that you can predict the output variables (Y) for that data. what are Inputs and Labels(Targets)?? for example addition of two numbers a=5,b=6 result =11, Inputs are 5,6 and Target is 11.

Supervised learning problems can be further grouped into regression and classification problems.
<p align="center">
  <img src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/1_HKJTVHXyTBsR-3ljuBL6qQ.png">
</p>



## Unsupervised Learning: 
Unsupervised Learning is a class of Machine Learning techniques to find the patterns in data. The data given to unsupervised algorithm are not labelled, which means only the input variables(X) are given with no corresponding output variables. In unsupervised learning, the algorithms are left to themselves to discover interesting structures in the data.

<p align="center">
  <img src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/4.png">
</p>

There are also different types for unsupervised learning like `Clustering` and `Anomaly detection` (clustering is pretty famous)

<p align="center">
  <img src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/5.png">
</p>

### Clustering: 
This is a type of problem where we group similar things together.
Bit similar to multi class classification but here we don‚Äôt provide the labels, the system understands from data itself and cluster the data.

Some examples are :
- given news articles,cluster into different types of news
- given a set of tweets ,cluster based on content of tweet
- given a set of images, cluster them into different objects

<p align="center">
  <img src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/6.png">
</p>


The training data does not include Targets here so we don‚Äôt tell the system where to go , the system has to understand itself from the data we give. Here training data is not structured (contains noisy data,unknown data and etc..)
 ```
 ex: A random articles from different pages

```

## Supervised Machine Learning Types:
- **Regression**
- **Classification**

<p align="center">
  <img width="560" height="350" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/1_HOgMR9PXYSfoasB8ctXLSQ.png">
</p>

### Regression: 
A regression problem is when the output variable is a `real value` (Continuous) , such as ‚Äúdollars‚Äù or ‚Äúweight‚Äù. 

Some examples are
- what is the price of house in a specific city?
- what is the value of the stock?
- how many total runs can be on board in a cricket game?

### Classification: 
A classification problem is when the output variable is a category, (Discrete) such as ‚Äúred‚Äù or ‚Äúblue‚Äù or ‚Äúdisease‚Äù and ‚Äúno disease‚Äù.

Some examples are :
- this mail is spam or not?
- will it rain today or not?
- is this picture a cat or not?
- Basically ‚ÄòYes/No‚Äô type questions called binary classification.

Other examples are :
- this mail is spam or important or promotion?
- is this picture a cat or a dog or a tiger?


## Algorithoms we are going to learn

- **Linear Regression**
- **Logistic Regression**
- **Decision Trees**
- **Support Vector Machine (SVM)**
- **Neural Networks**
- **Ensemble Learning**


<p align="center">
  <img width="460" height="400" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/1_QOS8cNI-A61sKwLJ8Nf8Iw.png">
</p>


``` 
                                                        Summary of Day 1
```
## Day 2:

## Linear Regression using Python:

Linear Regression is usually the first machine learning algorithm that every data scientist comes across. It is a simple model but everyone needs to master it as it lays the foundation for other machine learning algorithms.

## Where can Linear Regression be used? 
It is a very powerful technique and can be used to understand the factors that influence profitability. It can be used to forecast sales in the coming months by analyzing the sales data for previous months. It can also be used to gain various insights about customer behaviour. By the end of the blog we will build a model which looks like the below picture i.e, determine a line which best fits the data.



## What is Linear Regression
The objective of a linear regression model is to find a relationship between one or more features(independent variables) and a continuous target variable(dependent variable). When there is only feature it is called `Uni-variate Linear Regression` and if there are multiple features, it is called `Multivariate Linear Regression`.

<p align="center">
  <img width="460" height="450" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/LR.png">
</p>



## Hypothesis of Linear Regression

<p align="center">
  <img width="460" height="450" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/eq.png">
</p>

## How do we determine the best fit line? 

The line for which the the error between the predicted values and the observed values is minimum is called the best fit line or the regression line. These errors are also called as residuals. The `residuals` can be visualized by the vertical lines from the observed data value to the `regression` line.

<p align="center">
  <img width="460" height="400" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/LR2.png">
</p>

<p align="center">
  <img width="460" height="400" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/eq2.png">
</p>


 ## **Classification** 

## Classification Algorithms:



<p align="center">
  <img width="460" height="400" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/Regression_vs_Classification.png">
</p>
                                     
## What is a Classification Problem?
We identify problem as classification problem when independent variables are continuous in nature and dependent variable is in categorical form i.e. in classes like positive class and negative class. The real life example of classification example would be, to categorize the mail as spam or not spam, to categorize the tumor as malignant or benign and to categorize the transaction as fraudulent or genuine. All these problem‚Äôs answers are in categorical form i.e. Yes or No. and that is why they are two class classification problems.


<p align="center">
  <img width="460" height="400" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/1_-a_J9I0cr0BoJRc_6MhJog.png">
</p>


Although, sometime we come across more than 2 classes and still it is a classification problem. These types of problems are known as multi class classification problems.



## Logistic Regression: 

Logistic Regression is one of the basic and popular algorithm to solve a classification problem. It is named as ‚ÄòLogistic Regression‚Äô, because it‚Äôs underlying technique is quite the same as Linear Regression. The term ‚ÄúLogistic‚Äù is taken from the Logit function that is used in this method of classification.



## Why not use Linear Regression?
Suppose we have a data of tumor size vs its malignancy. As it is a classification problem, if we plot, we can see, all the values will lie on 0 and 1. And if we fit best found regression line, by assuming the threshold at 0.5, we can do line pretty reasonable job.


<p align="center">
  <img width="460" height="400" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/22.png">
</p>

We can decide the point on the x axis from where all the values lie to its left side are considered as negative class and all the values lie to its right side are positive class.

<p align="center">
  <img width="460" height="400" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/33.jpeg">
</p>

But what if there is an outlier in the data. Things would get pretty messy. For example, for 0.5 threshold,


<p align="center">
  <img width="460" height="400" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/44.png">
</p>


If we fit best found regression line, it still won‚Äôt be enough to decide any point by which we can differentiate classes. It will put some positive class examples into negative class. The green dotted line (Decision Boundary) is dividing malignant tumors from benign tumors but the line should have been at a yellow line which is clearly dividing the positive and negative examples. So just a single outlier is disturbing the whole linear regression predictions. And that is where logistic regression comes into a picture.


## Logistic Regression Algorithm
As discussed earlier, to deal with outliers, Logistic Regression uses Sigmoid function.
An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a Sigmoid function, which takes any real value between zero and one. It is defined as

<p align="center">
  <img width="260" height="200" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/55.png">
</p>



And if we plot it, the graph will be S curve,

<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/66.png">
</p>

Let‚Äôs consider t as linear function in a univariate regression model.

<p align="center">
  <img width="260" height="200" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/77.png">
</p>

So the Logistic Equation will become:

<p align="center">
  <img width="260" height="200" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/88.png">
</p>

Now, when logistic regression model come across an outlier, it will take care of it.

But sometime it will shift its y axis to left or right depending on outliers positions.

## What is Decision Boundary?

Decision boundary helps to differentiate probabilities into positive class and negative class.

## Non Linear Decision Boundary
<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/db.png">
</p>

## Linear Decision Boundary

<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/db2.png">
</p>

## Classification and Regression Trees
Decision Trees are an important type of algorithm for predictive modeling machinelearning.

The representation of the decision tree model is a binary tree. This is your binary tree from algorithms and data structures, nothing too fancy. Each node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).

<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/tree.png">
</p>

The leaf nodes of the tree contain an output variable (y) which is used to make a prediction. Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.

Trees are fast to learn and very fast for making predictions. They are also often accurate for a broad range of problems and do not require any special preparation for your data.


## Naive Bayes

Naive Bayes is a simple but surprisingly powerful algorithm for predictive modeling.

The model is comprised of two types of probabilities that can be calculated directly from your training data: 1) The probability of each class; and 2) The conditional probability for each class given each x value. Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem. When your data is real-valued it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities.

<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/bayes.png">
</p>


Naive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems.

## K-Nearest Neighbors
The KNN algorithm is very simple and very effective. The model representation for KNN is the entire training dataset. Simple right?

Predictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression problems, this might be the mean output variable, for classification problems this might be the mode (or most common) class value.

The trick is in how to determine the similarity between the data instances. The simplest technique if your attributes are all of the same scale (all in inches for example) is to use the Euclidean distance, a number you can calculate directly based on the differences between each input variable.

<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/knn.png">
</p>

KNN can require a lot of memory or space to store all of the data, but only performs a calculation (or learn) when a prediction is needed, just in time. You can also update and curate your training instances over time to keep predictions accurate.

The idea of distance or closeness can break down in very high dimensions (lots of input variables) which can negatively affect the performance of the algorithm on your problem. This is called the curse of dimensionality. It suggests you only use those input variables that are most relevant to predicting the output variable.

## Learning Vector Quantization

A downside of K-Nearest Neighbors is that you need to hang on to your entire training dataset. The Learning Vector Quantization algorithm (or LVQ for short) is an artificial neural network algorithm that allows you to choose how many training instances to hang onto and learns exactly what those instances should look like.

<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/vector.png">
</p>

The representation for LVQ is a collection of codebook vectors. These are selected randomly in the beginning and adapted to best summarize the training dataset over a number of iterations of the learning algorithm. After learned, the codebook vectors can be used to make predictions just like K-Nearest Neighbors. The most similar neighbor (best matching codebook vector) is found by calculating the distance between each codebook vector and the new data instance. The class value or (real value in the case of regression) for the best matching unit is then returned as the prediction. Best results are achieved if you rescale your data to have the same range, such as between 0 and 1.

If you discover that KNN gives good results on your dataset try using LVQ to reduce the memory requirements of storing the entire training dataset.

## Support Vector Machines

Support Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.

A hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions, you can visualize this as a line and let‚Äôs assume that all of our input points can be completely separated by this line. The SVM learning algorithm finds the coefficients that results in the best separation of the classes by the hyperplane.

<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/svm.jpeg">
</p>

The distance between the hyperplane and the closest data points is referred to as the margin. The best or optimal hyperplane that can separate the two classes is the line that has the largest margin. Only these points are relevant in defining the hyperplane and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane. In practice, an optimization algorithm is used to find the values for the coefficients that maximizes the margin.

SVM might be one of the most powerful out-of-the-box classifiers and worth trying on your dataset.

## Bagging and Random Forest

Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.

The bootstrap is a powerful statistical method for estimating a quantity from a data sample. Such as a mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value.

In bagging, the same approach is used, but instead for estimating entire statistical models, most commonly decision trees. Multiple samples of your training data are taken then models are constructed for each data sample. When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimate of the true output value.


<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/forest.png">
</p>

Random forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness.

The models created for each sample of the data are therefore more different than they otherwise would be, but still accurate in their unique and different ways. Combining their predictions results in a better estimate of the true underlying output value.

If you get good results with an algorithm with high variance (like decision trees), you can often get better results by bagging that algorithm.


## Boosting and AdaBoost

Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.

AdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting. Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.

<p align="center">
  <img width="460" height="300" src="https://github.com/NeloyNSU/Workshop-on-Machine-Learning-2018/blob/master/Images/boosting.jpeg">
</p>

AdaBoost is used with short decision trees. After the first tree is created, the performance of the tree on each training instance is used to weight how much attention the next tree that is created should pay attention to each training instance. Training data that is hard to predict is given more weight, whereas easy to predict instances are given less weight. Models are created sequentially one after the other, each updating the weights on the training instances that affect the learning performed by the next tree in the sequence. After all the trees are built, predictions are made for new data, and the performance of each tree is weighted by how accurate it was on training data.

Because so much attention is put on correcting mistakes by the algorithm it is important that you have clean data with outliers removed.


## Some Important Methodologies:
- **DataSets**
- **Prediction**
- **Model Validation**


## DataSets:

There are three types of data sets ‚Äì Training, Dev and Test that are used at various stage of development. Training dataset is the largest of three of them, while test data functions as seal of approval and you don‚Äôt need to use till the end of the development.

## What is Test Dataset in ML?

This is the data typically used to provide an unbiased evaluation of the final that are completed and fit on the training dataset. Actually, such data is used for testing the model whether it is responding or working appropriately or not.

## Prediction
### What does Prediction mean in Machine Learning?
‚ÄúPrediction‚Äù refers to the output of an algorithm after it has been trained on a historical dataset and applied to new data when you‚Äôre trying to forecast the likelihood of a particular outcome, such as whether or not a customer will churn in 30 days. The algorithm will generate probable values for an unknown variable for each record in the new data, allowing the model builder to identify what that value will most likely be.

The word ‚Äúprediction‚Äù can be misleading. In some cases, it really does mean that you are predicting a future outcome, such as when you‚Äôre using machine learning to determine the next best action in a marketing campaign. Other times, though, the ‚Äúprediction‚Äù has to do with, for example, whether or not a transaction that already occurred was fraud. In that case, the transaction already happened, but you‚Äôre making an educated guess about whether or not it was legitimate, allowing you to take the appropriate action.

### Why are Predictions important?

Machine learning model predictions allow businesses to make highly accurate guesses as to the likely outcomes of a question based on historical data, which can be about all kinds of things ‚Äì customer churn likelihood, possible fraudulent activity, and more. These provide the business with insights that result in tangible business value. For example, if a model predicts a customer is likely to churn, the business can target them with specific communications and outreach that will prevent the loss of that customer.


## Model Validation

### Metrics

Choice of metrics influences how the performance of machine learning algorithms is measured and compared. They influence how you weight the importance of different characteristics in the results and your ultimate choice of which algorithm to choose. In this project, as both Regression and Classification problem was handled so both types of matrices were used. 

### Regression Problem Metrices:
- **Mean Absolute Error.**
- **Mean Squared Error.**
- **R^2**

### Mean Absolute Error
The Mean Absolute Error (or MAE) is the sum of the absolute differences between predictions and actual values. It gives an idea of how wrong the predictions were.
The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).

### Mean Squared Error
The Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of the error.
Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the Root Mean Squared Error (or RMSE).

### R^2 Metric
The R^2 (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature, this measure is called the coefficient of determination.

## Classification Problem Metrices:
- **Classification Accuracy.**
- **Area Under ROC Curve.**

### Classification Accuracy
Classification accuracy is the number of correct predictions made as a ratio of all predictions made.
This is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there is an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case.


### Area Under ROC Curve
The area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.
The AUC represents a model‚Äôs ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. 
ROC can be broken down into sensitivity and specificity. A binary classification problem is really a trade-off between sensitivity and specificity.

- Sensitivity is the true positive rate also called the recall. It is the number of instances from the positive (first) class that actually predicted correctly.
- Specificity is also called the true negative rate. Is the number of instances from the negative class (second) class that was actually predicted correctly.
